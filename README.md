# ‚ú® Highlights
- Architecture (MLLM): CLIP-ViT multi-layer features ‚Üí Q-Former‚Äìstyle Projector ‚Üí Zephyr-7B-Œ≤ backbone for text generation (answer + rationale). 
- Visual Prompting (alpha-blending): overlay boxes, masks, arrows, scribbles, etc., directly on the image to steer attention‚Äîsimple, fast, and effective. 
- Any-Resolution (anyres) input: tile high-res crops + a low-res global image to preserve detail beyond a fixed 336√ó336 input. 
- Training setup (Stage-1): Zephyr-7B-Œ≤ with CLIP ViT-L/14-336; anyres enabled; effective batch size 64 on 4√ó A100; max length 2048. 
- Component ablations: Multi-layer Feature Fusion reduces loss vs. single-layer selection; Q-Former projector converges faster and lower loss than 2-layer MLP.

# üß† Model Overview
Vision-Zephyr follows the decoder-only MLLM recipe: CLIP-ViT encodes the image (multi-layer features), a Q-Former‚Äìstyle projector maps vision tokens into the LLM token space, and Zephyr-7B-Œ≤ performs autoregressive decoding to produce answers and rationales. 

# üöÄ Quickstart
**System requirements**:
	- GPU: NVIDIA A100 (ho·∫∑c GPU h·ªó tr·ª£ bfloat16)
 	- VRAM: ‚â• 40GB.

**Docker**
```bash
docker pull tyzen/viszephyr:cli
```
**Git Clone**
```bash
git clone https://github.com/baohuyvanba/Vision-Zephyr.git
cd Vision-Zephyr
```
**Installing Package**
```bash
pip install -e .
```
**Download Model's Checkpoints**
```bash
cd checkpoints/vis-zephyr-7b-v1-pretrain
wget -c https://huggingface.co/datasets/tyzen27/vcr_finetune/resolve/main/checkpoints/vis-zephyr-7b-v1-pretrain/mm_projector.bin
```

**CLI**
```bash
chmod +x script/cli.sh
script/cli.sh
```

# üìÇ Data
- LAIONCCSBU BLIP-Caption Concept-balanced 558K (subset used during Stage-1). 
- VCR Validation for downstream evaluation (~23K items; 26,5K processed entries). 
- MMBench Validation for broader multimodal capabilities (optional).

# ü§ù Acknowledgements
Built on top of Zephyr-7B-Œ≤ and CLIP ViT-L/14-336, with design inspirations from ViP-LLaVA and anyres tiling strategies.
